{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import Tensor\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import tqdm\n",
    "from tests.utils.data import PairData\n",
    "import random\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sgmatch.models.GraphSim import GraphSim, GraphSim_v2\n",
    "from tests.utils.dataset import load_dataset\n",
    "from tests.utils.parser import parser\n",
    "from tests.utils.data import PairData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset retreival or generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(x,edge_index):\n",
    "    adjmat={}\n",
    "    reached={}\n",
    "\n",
    "    for _ in range(x.shape[0]):\n",
    "        adjmat[_]=[]\n",
    "        reached[_]=False\n",
    "\n",
    "    for each in torch.transpose(edge_index,0,1):\n",
    "        adjmat[int(each[0])].append(int(each[1]))\n",
    "    \n",
    "    visited=[]\n",
    "    visited.append(0)\n",
    "    reached[0]=True\n",
    "    output=[]\n",
    "    while len(visited)!=0:\n",
    "        top=visited[0]\n",
    "        for each in adjmat[top]:\n",
    "            if not reached[each] :\n",
    "                visited.append(each)\n",
    "                reached[each]=True\n",
    "        output.append(top)\n",
    "        visited.remove(top)\n",
    "    \n",
    "    mapping=dict(zip(output,[x for x in range(x.shape[0])]))\n",
    "    \n",
    "    return x[output], torch.Tensor([[mapping[int(e)] for e in list(edge_index[0])],[mapping[int(e)] for e in list(edge_index[1])]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_pairs(train_dataset, test_dataset) -> Tuple[List]:\n",
    "    train_graph_pairs = []\n",
    "    with tqdm.tqdm(total=len(train_dataset)**2, desc='Train graph pairs completed: ') as bar:\n",
    "        for idx1, graph1 in enumerate(train_dataset):\n",
    "            for idx2, graph2 in enumerate(train_dataset):\n",
    "                if idx1 == idx2:\n",
    "                    continue\n",
    "                # Initializing Data\n",
    "                x_s, edge_index_s=bfs(graph1.x,graph1.edge_index)\n",
    "                x_t, edge_index_t=bfs(graph2.x,graph2.edge_index)\n",
    "\n",
    "                #Max padding as stated in paper\n",
    "                if graph1.num_nodes < graph2.num_nodes:\n",
    "                    x_s=torch.cat((x_s,torch.zeros(graph2.num_nodes-graph1.num_nodes,graph1.x.shape[1])),dim=0)\n",
    "                else:\n",
    "                    x_t=torch.cat((x_t,torch.zeros(graph1.num_nodes-graph2.num_nodes,graph1.x.shape[1])),dim=0)\n",
    "\n",
    "                norm_ged = train_dataset.norm_ged[graph1.i, graph2.i]\n",
    "                graph_sim = torch.exp(-norm_ged).unsqueeze(-1)\n",
    "                \n",
    "                # Making Graph Pair\n",
    "                if isinstance(x_s, Tensor) and isinstance(x_t, Tensor):\n",
    "                    graph_pair = PairData(edge_index_s=edge_index_s, x_s=x_s,\n",
    "                                        edge_index_t=edge_index_t, x_t=x_t,\n",
    "                                        y=graph_sim)\n",
    "                    \n",
    "                    # Saving all the Graph Pairs to the List for Batching and Data Loading\n",
    "                    train_graph_pairs.append(graph_pair)\n",
    "            bar.update(len(train_dataset))\n",
    "    \n",
    "    test_graph_pairs = []\n",
    "    with tqdm.tqdm(total=len(test_dataset)*len(train_dataset), desc='Test graph pairs completed: ') as bar:\n",
    "        for graph1 in test_dataset:\n",
    "            for graph2 in train_dataset:\n",
    "                # Initializing Data\n",
    "                x_s, edge_index_s=bfs(graph1.x,graph1.edge_index)\n",
    "                x_t, edge_index_t=bfs(graph2.x,graph2.edge_index)\n",
    "\n",
    "                #Max padding as stated in paper\n",
    "                if graph1.num_nodes < graph2.num_nodes:\n",
    "                    x_s=torch.cat((x_s,torch.zeros(graph2.num_nodes-graph1.num_nodes,graph1.x.shape[1])),dim=0)\n",
    "                else:\n",
    "                    x_t=torch.cat((x_t,torch.zeros(graph1.num_nodes-graph2.num_nodes,graph1.x.shape[1])),dim=0)\n",
    "                    \n",
    "                norm_ged = train_dataset.norm_ged[graph1.i, graph2.i]\n",
    "                graph_sim = torch.exp(-norm_ged).unsqueeze(-1)\n",
    "                \n",
    "                # Making Graph Pair\n",
    "                if isinstance(x_s, Tensor) and isinstance(x_t, Tensor):\n",
    "                    graph_pair = PairData(edge_index_s=edge_index_s, x_s=x_s,\n",
    "                                        edge_index_t=edge_index_t, x_t=x_t,\n",
    "                                        y=graph_sim)\n",
    "                \n",
    "                    # Saving all the Graph Pairs to the List for Batching and Data Loading\n",
    "                    test_graph_pairs.append(graph_pair)\n",
    "            bar.update(len(train_dataset))\n",
    "    \n",
    "    return train_graph_pairs, test_graph_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, loss_criterion, optimizer, device, num_epochs=10):\n",
    "    batch_train_loss_sum = 0\n",
    "    batch_val_loss_sum = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with tqdm.tqdm(total=len(train_loader), desc='Train batches completed: ') as bar:\n",
    "            for batch_idx, train_batch in enumerate(train_loader):\n",
    "                model.train()\n",
    "                train_batch = train_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred_sim = model(train_batch.x_s, train_batch.edge_index_s, train_batch.x_t, \n",
    "                                train_batch.edge_index_t, train_batch.x_s_batch, train_batch.x_t_batch)\n",
    "                mean_batch_loss = loss_criterion(pred_sim, train_batch.y)\n",
    "                # Compute Gradients via Backpropagation\n",
    "                mean_batch_loss.backward()\n",
    "                # Update Parameters\n",
    "                optimizer.step()\n",
    "                batch_train_loss_sum += mean_batch_loss.item()*len(train_batch)\n",
    "                \n",
    "                bar.update(1)\n",
    "\n",
    "        with tqdm.tqdm(total=len(val_loader), desc='Validation batches completed: ') as bar:\n",
    "            for batch_idx, val_batch in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_batch = val_batch.to(device)\n",
    "                    pred_sim = model(val_batch.x_s, val_batch.edge_index_s, \n",
    "                            val_batch.x_t, val_batch.edge_index_t, val_batch.x_s_batch, val_batch.x_t_batch)\n",
    "                    mean_val_loss = loss_criterion(pred_sim, val_batch.y)\n",
    "                    batch_val_loss_sum += mean_val_loss.item()*len(val_batch)\n",
    "\n",
    "                bar.update(1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache() \n",
    "    \n",
    "        # Printing Epoch Summary\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs} | Per Graph Train MSE: {batch_train_loss_sum / len(train_loader.dataset)} | Mean batch loss :{mean_batch_loss} \\n   |Per Graph Validation MSE: {batch_val_loss_sum / len(val_loader.dataset)}| Mean_val_loss: {mean_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train graph pairs completed:   0%|          | 0/313600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train graph pairs completed:   2%|â–         | 6720/313600 [00:06<04:58, 1027.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\saiki\\AppData\\Local\\Temp\\ipykernel_18352\\334977845.py\", line 12, in <module>\n",
      "    train_graph_pairs, test_graph_pairs = create_graph_pairs(train_dataset, test_dataset)\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\saiki\\AppData\\Local\\Temp\\ipykernel_18352\\2907749329.py\", line 10, in create_graph_pairs\n",
      "    x_t, edge_index_t=bfs(graph2.x,graph2.edge_index)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\saiki\\AppData\\Local\\Temp\\ipykernel_18352\\4085205553.py\", line -1, in bfs\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_path=\"./data\"\n",
    "train_batch_size=128\n",
    "val_batch_size=64\n",
    "test_batch_size=256\n",
    "learning_rate=0.01\n",
    "\n",
    "train_dataset = load_dataset(dpath=data_path+\"/aids/\", name=\"GED\", category=\"AIDS700nef\", train=True)\n",
    "test_dataset = load_dataset(dpath=data_path+\"/aids/\", name=\"GED\", category=\"AIDS700nef\", train=False)\n",
    "\n",
    "train_graph_pairs, test_graph_pairs = create_graph_pairs(train_dataset, test_dataset)\n",
    "torch.save(train_graph_pairs, data_path+\"/aids/graph_pairs/train_graph_pairs_Graphsim.pt\")\n",
    "torch.save(test_graph_pairs, data_path+\"/aids/graph_pairs/test_graph_pairs_Graphsim.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_pairs, test_graph_pairs = torch.load(data_path+\"/aids/graph_pairs/train_graph_pairs_Graphsim.pt\"),\\\n",
    "                                             torch.load(data_path+\"/aids/graph_pairs/test_graph_pairs_Graphsim.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_path=\"./data\"\n",
    "train_batch_size=128\n",
    "val_batch_size=64\n",
    "test_batch_size=256\n",
    "learning_rate=0.01\n",
    "\n",
    "\n",
    "    \n",
    "val_idxs = np.random.randint(len(train_graph_pairs), size=len(test_graph_pairs))\n",
    "val_graph_pairs = [train_graph_pairs[idx] for idx in val_idxs]\n",
    "train_idxs = set(range(len(train_graph_pairs))) - set(val_idxs)\n",
    "train_graph_pairs = [train_graph_pairs[idx] for idx in train_idxs]\n",
    "del val_idxs, train_idxs\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_graph_pairs, batch_size = train_batch_size, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "val_loader = DataLoader(val_graph_pairs, batch_size = val_batch_size, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "test_loader = DataLoader(test_graph_pairs, batch_size = test_batch_size, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "\n",
    "class CustomModuleList(torch.nn.Module):\n",
    "    def __init__(self, module_list):\n",
    "        super(CustomModuleList, self).__init__()\n",
    "        self.module_list = torch.nn.ModuleList(module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "convo_filters=torch.nn.ModuleList([\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=1),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    ])\n",
    "\n",
    "model = GraphSim(input_dim=train_loader.dataset[0].x_s.shape[-1],conv_filters=CustomModuleList(convo_filters)).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches completed:   0%|          | 0/113 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GCNConv.forward() missing 1 required positional argument: 'edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saiki\\IITB\\BTP1\\graphretrievaltoolkit-main\\graphretrievaltoolkit-main\\test_GraphSim.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/test_GraphSim.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(train_loader, val_loader, model, criterion, optimizer, device)\n",
      "\u001b[1;32mc:\\Users\\saiki\\IITB\\BTP1\\graphretrievaltoolkit-main\\graphretrievaltoolkit-main\\test_GraphSim.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/test_GraphSim.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_batch \u001b[39m=\u001b[39m train_batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/test_GraphSim.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/test_GraphSim.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m pred_sim \u001b[39m=\u001b[39m model(train_batch\u001b[39m.\u001b[39;49mx_s, train_batch\u001b[39m.\u001b[39;49medge_index_s, train_batch\u001b[39m.\u001b[39;49mx_t, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/test_GraphSim.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                 train_batch\u001b[39m.\u001b[39;49medge_index_t, train_batch\u001b[39m.\u001b[39;49mx_s_batch, train_batch\u001b[39m.\u001b[39;49mx_t_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/test_GraphSim.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m mean_batch_loss \u001b[39m=\u001b[39m loss_criterion(pred_sim, train_batch\u001b[39m.\u001b[39my)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/test_GraphSim.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Compute Gradients via Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\saiki\\IITB\\BTP1\\graphretrievaltoolkit-main\\graphretrievaltoolkit-main\\sgmatch\\models\\GraphSim.py:126\u001b[0m, in \u001b[0;36mGraphSim.forward\u001b[1;34m(self, x_i, x_j, edge_index_i, edge_index_j, batch_i, batch_j)\u001b[0m\n\u001b[0;32m    123\u001b[0m gnn_activation \u001b[39m=\u001b[39m ACTIVATION_LAYERS[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgnn_activation]\n\u001b[0;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m layer_num, gnn_layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgnn_layers):\n\u001b[0;32m    125\u001b[0m     \u001b[39m# Pass through GNN\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     x_i \u001b[39m=\u001b[39m gnn_layer(x_i)\n\u001b[0;32m    127\u001b[0m     x_j \u001b[39m=\u001b[39m gnn_layer(x_j)\n\u001b[0;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m layer_num \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgnn_layers)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: GCNConv.forward() missing 1 required positional argument: 'edge_index'"
     ]
    }
   ],
   "source": [
    "train(train_loader, val_loader, model, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 29]),\n",
       " tensor([[0, 1, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 7, 8, 9, 9, 9, 9],\n",
       "         [1, 0, 3, 5, 6, 9, 1, 9, 1, 7, 1, 5, 9, 9, 2, 4, 7, 8]]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].x.shape,train_dataset[0].edge_index\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
