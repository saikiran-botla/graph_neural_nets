{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import tqdm\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch.functional import Tensor\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from sgmatch.models.SimGNN import SimGNN\n",
    "from tests.utils.dataset import load_dataset\n",
    "from tests.utils.parser import parser\n",
    "from tests.utils.data import PairData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_pairs(train_dataset, test_dataset) -> Tuple[List]:\n",
    "    train_graph_pairs = []\n",
    "    with tqdm.tqdm(total=len(train_dataset)**2, desc='Train graph pairs completed: ') as bar:\n",
    "        for idx1, graph1 in enumerate(train_dataset):\n",
    "            for idx2, graph2 in enumerate(train_dataset):\n",
    "                if idx1 == idx2:\n",
    "                    continue\n",
    "                # Initializing Data\n",
    "                edge_index_s = graph1.edge_index\n",
    "                x_s = graph1.x\n",
    "\n",
    "                edge_index_t = graph2.edge_index\n",
    "                x_t = graph2.x\n",
    "\n",
    "                norm_ged = train_dataset.norm_ged[graph1.i, graph2.i]\n",
    "                graph_sim = torch.exp(-norm_ged).unsqueeze(-1)\n",
    "                \n",
    "                # Making Graph Pair\n",
    "                if isinstance(x_s, Tensor) and isinstance(x_t, Tensor):\n",
    "                    graph_pair = PairData(edge_index_s=edge_index_s, x_s=x_s,\n",
    "                                        edge_index_t=edge_index_t, x_t=x_t,\n",
    "                                        y=graph_sim)\n",
    "                    \n",
    "                    # Saving all the Graph Pairs to the List for Batching and Data Loading\n",
    "                    train_graph_pairs.append(graph_pair)\n",
    "            bar.update(len(train_dataset))\n",
    "    \n",
    "    test_graph_pairs = []\n",
    "    with tqdm.tqdm(total=len(test_dataset)*len(train_dataset), desc='Test graph pairs completed: ') as bar:\n",
    "        for graph1 in test_dataset:\n",
    "            for graph2 in train_dataset:\n",
    "                # Initializing Data\n",
    "                edge_index_s = graph1.edge_index\n",
    "                x_s = graph1.x\n",
    "                edge_index_t = graph2.edge_index\n",
    "                x_t = graph2.x\n",
    "\n",
    "                norm_ged = train_dataset.norm_ged[graph1.i, graph2.i]\n",
    "                graph_sim = torch.exp(-norm_ged).unsqueeze(-1)\n",
    "                \n",
    "                # Making Graph Pair\n",
    "                if isinstance(x_s, Tensor) and isinstance(x_t, Tensor):\n",
    "                    graph_pair = PairData(edge_index_s=edge_index_s, x_s=x_s,\n",
    "                                        edge_index_t=edge_index_t, x_t=x_t,\n",
    "                                        y=graph_sim)\n",
    "                \n",
    "                    # Saving all the Graph Pairs to the List for Batching and Data Loading\n",
    "                    test_graph_pairs.append(graph_pair)\n",
    "            bar.update(len(train_dataset))\n",
    "    \n",
    "    return train_graph_pairs, test_graph_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, loss_criterion, optimizer, device, num_epochs=10):\n",
    "    batch_train_loss_sum = 0\n",
    "    batch_val_loss_sum = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with tqdm.tqdm(total=len(train_loader), desc='Train batches completed: ') as bar:\n",
    "            for batch_idx, train_batch in enumerate(train_loader):\n",
    "                model.train()\n",
    "                train_batch = train_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred_sim = model(train_batch.x_s, train_batch.edge_index_s, train_batch.x_t, \n",
    "                                train_batch.edge_index_t, train_batch.x_s_batch, train_batch.x_t_batch)\n",
    "                mean_batch_loss = loss_criterion(pred_sim, train_batch.y)\n",
    "                # Compute Gradients via Backpropagation\n",
    "                mean_batch_loss.backward()\n",
    "                # Update Parameters\n",
    "                optimizer.step()\n",
    "                batch_train_loss_sum += mean_batch_loss.item()*len(train_batch)\n",
    "                \n",
    "                bar.update(1)\n",
    "\n",
    "        with tqdm.tqdm(total=len(val_loader), desc='Validation batches completed: ') as bar:\n",
    "            for batch_idx, val_batch in enumerate(val_loader):\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_batch = val_batch.to(device)\n",
    "                    pred_sim = model(val_batch.x_s, val_batch.edge_index_s, \n",
    "                            val_batch.x_t, val_batch.edge_index_t, val_batch.x_s_batch, val_batch.x_t_batch)\n",
    "                    mean_val_loss = loss_criterion(pred_sim, val_batch.y)\n",
    "                    batch_val_loss_sum += mean_val_loss.item()*len(val_batch)\n",
    "\n",
    "                bar.update(1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache() \n",
    "    \n",
    "        # Printing Epoch Summary\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs} | Per Graph Train MSE: {batch_train_loss_sum / len(train_loader.dataset)} | Mean batch loss :{mean_batch_loss} \\n   |Per Graph Validation MSE: {batch_val_loss_sum / len(val_loader.dataset)}| Mean_val_loss: {mean_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_path=\"./data\"\n",
    "train_batch_size=128\n",
    "val_batch_size=64\n",
    "test_batch_size=256\n",
    "learning_rate=0.01\n",
    "\n",
    "train_dataset = load_dataset(dpath=data_path+\"/aids/\", name=\"GED\", category=\"AIDS700nef\", train=True)\n",
    "test_dataset = load_dataset(dpath=data_path+\"/aids/\", name=\"GED\", category=\"AIDS700nef\", train=False)\n",
    "\n",
    "train_ged_table = train_dataset.ged[:train_dataset.data.i[-1]+1, :train_dataset.data.i[-1]+1]\n",
    "test_ged_table = test_dataset.ged[train_dataset.data.i[-1]+1:, train_dataset.data.i[-1]+1:]\n",
    "\n",
    "\n",
    "train_graph_pairs, test_graph_pairs = torch.load(data_path+\"/aids/graph_pairs/train_graph_pairs.pt\"),\\\n",
    "                                              torch.load(data_path+\"/aids/graph_pairs/test_graph_pairs.pt\")\n",
    "\n",
    "val_idxs = np.random.randint(len(train_graph_pairs), size=len(test_graph_pairs))\n",
    "val_graph_pairs = [train_graph_pairs[idx] for idx in val_idxs]\n",
    "train_idxs = set(range(len(train_graph_pairs))) - set(val_idxs)\n",
    "train_graph_pairs = [train_graph_pairs[idx] for idx in train_idxs]\n",
    "del val_idxs, train_idxs\n",
    "\n",
    "train_loader = DataLoader(train_graph_pairs, batch_size = 128, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "val_loader = DataLoader(val_graph_pairs, batch_size = 64, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "test_loader = DataLoader(test_graph_pairs, batch_size = 256, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "\n",
    "model = SimGNN(input_dim=train_loader.dataset[0].x_s.shape[-1]).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(edge_index=[2, 9898], i=[560], num_nodes=4991, x=[4991, 29]),\n",
       " defaultdict(dict,\n",
       "             {'edge_index': tensor([   0,   18,   34,   54,   76,   90,   96,  116,  134,  152,  164,  180,\n",
       "                       198,  216,  238,  250,  262,  272,  292,  306,  316,  332,  350,  372,\n",
       "                       386,  392,  404,  422,  436,  450,  470,  488,  502,  514,  532,  552,\n",
       "                       564,  582,  602,  624,  638,  652,  664,  678,  698,  718,  732,  752,\n",
       "                       768,  778,  798,  818,  834,  852,  866,  880,  904,  922,  934,  950,\n",
       "                       956,  962,  978,  992, 1010, 1024, 1038, 1056, 1066, 1082, 1104, 1116,\n",
       "                      1136, 1150, 1164, 1180, 1200, 1220, 1234, 1256, 1274, 1294, 1314, 1330,\n",
       "                      1352, 1360, 1378, 1400, 1416, 1436, 1454, 1472, 1486, 1504, 1522, 1540,\n",
       "                      1550, 1568, 1590, 1604, 1620, 1636, 1656, 1676, 1698, 1720, 1742, 1752,\n",
       "                      1768, 1784, 1798, 1814, 1830, 1848, 1862, 1880, 1896, 1914, 1932, 1952,\n",
       "                      1970, 1990, 2012, 2026, 2048, 2066, 2084, 2104, 2124, 2142, 2158, 2178,\n",
       "                      2202, 2220, 2238, 2254, 2268, 2284, 2302, 2320, 2334, 2354, 2374, 2396,\n",
       "                      2416, 2430, 2448, 2466, 2482, 2504, 2520, 2534, 2552, 2568, 2586, 2600,\n",
       "                      2620, 2636, 2656, 2674, 2692, 2706, 2722, 2738, 2754, 2768, 2782, 2800,\n",
       "                      2820, 2838, 2854, 2872, 2888, 2908, 2930, 2952, 2974, 2988, 3014, 3034,\n",
       "                      3046, 3064, 3084, 3098, 3114, 3126, 3146, 3162, 3182, 3202, 3220, 3240,\n",
       "                      3260, 3280, 3298, 3316, 3338, 3360, 3382, 3404, 3416, 3438, 3456, 3472,\n",
       "                      3492, 3510, 3530, 3548, 3570, 3576, 3598, 3610, 3626, 3648, 3658, 3680,\n",
       "                      3696, 3712, 3730, 3748, 3770, 3798, 3818, 3832, 3852, 3874, 3888, 3910,\n",
       "                      3926, 3946, 3964, 3980, 4002, 4022, 4040, 4056, 4074, 4092, 4110, 4134,\n",
       "                      4156, 4162, 4180, 4202, 4224, 4242, 4258, 4278, 4300, 4304, 4324, 4340,\n",
       "                      4346, 4352, 4366, 4382, 4402, 4418, 4436, 4452, 4466, 4484, 4504, 4524,\n",
       "                      4536, 4554, 4576, 4598, 4612, 4632, 4648, 4666, 4678, 4696, 4714, 4734,\n",
       "                      4756, 4776, 4794, 4814, 4830, 4850, 4872, 4886, 4898, 4920, 4942, 4958,\n",
       "                      4978, 4996, 5012, 5032, 5054, 5066, 5086, 5108, 5120, 5140, 5152, 5154,\n",
       "                      5174, 5190, 5206, 5222, 5242, 5262, 5274, 5294, 5310, 5332, 5350, 5370,\n",
       "                      5384, 5402, 5416, 5434, 5456, 5478, 5494, 5514, 5536, 5558, 5578, 5590,\n",
       "                      5606, 5630, 5646, 5666, 5684, 5704, 5724, 5742, 5758, 5774, 5792, 5810,\n",
       "                      5830, 5850, 5872, 5892, 5910, 5932, 5954, 5972, 5984, 6000, 6010, 6030,\n",
       "                      6050, 6068, 6084, 6094, 6112, 6130, 6146, 6164, 6170, 6182, 6202, 6220,\n",
       "                      6236, 6256, 6276, 6296, 6312, 6328, 6346, 6368, 6382, 6404, 6426, 6446,\n",
       "                      6454, 6472, 6488, 6508, 6528, 6544, 6564, 6584, 6606, 6624, 6644, 6666,\n",
       "                      6686, 6706, 6724, 6744, 6762, 6778, 6790, 6802, 6818, 6834, 6854, 6876,\n",
       "                      6892, 6912, 6930, 6946, 6962, 6982, 7004, 7022, 7036, 7056, 7078, 7096,\n",
       "                      7110, 7130, 7142, 7160, 7176, 7194, 7210, 7230, 7240, 7256, 7272, 7294,\n",
       "                      7310, 7330, 7344, 7360, 7378, 7392, 7408, 7424, 7444, 7460, 7478, 7496,\n",
       "                      7516, 7536, 7556, 7576, 7588, 7602, 7616, 7638, 7652, 7658, 7670, 7686,\n",
       "                      7704, 7726, 7746, 7762, 7782, 7802, 7820, 7838, 7856, 7874, 7894, 7912,\n",
       "                      7928, 7944, 7964, 7980, 8002, 8022, 8042, 8062, 8082, 8100, 8120, 8138,\n",
       "                      8154, 8170, 8192, 8210, 8230, 8250, 8268, 8284, 8300, 8318, 8338, 8356,\n",
       "                      8376, 8390, 8412, 8426, 8446, 8466, 8484, 8504, 8524, 8544, 8562, 8582,\n",
       "                      8600, 8620, 8640, 8660, 8680, 8698, 8714, 8728, 8750, 8772, 8790, 8808,\n",
       "                      8828, 8842, 8864, 8884, 8904, 8922, 8940, 8960, 8978, 8998, 9018, 9038,\n",
       "                      9058, 9078, 9098, 9120, 9136, 9158, 9174, 9194, 9214, 9232, 9250, 9268,\n",
       "                      9286, 9306, 9320, 9336, 9354, 9376, 9396, 9414, 9434, 9454, 9474, 9494,\n",
       "                      9510, 9528, 9548, 9568, 9588, 9610, 9632, 9648, 9666, 9686, 9706, 9724,\n",
       "                      9744, 9766, 9788, 9808, 9826, 9842, 9860, 9878, 9898]),\n",
       "              'i': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "                       14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "                       28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "                       42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "                       56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "                       70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "                       84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "                       98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "                      112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "                      126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "                      140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "                      154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "                      168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "                      182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "                      196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "                      210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "                      224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "                      238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "                      252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
       "                      266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
       "                      280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "                      294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "                      308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "                      322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
       "                      336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "                      350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "                      364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "                      378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "                      392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "                      406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "                      420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
       "                      434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
       "                      448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
       "                      462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "                      476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "                      490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
       "                      504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
       "                      518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
       "                      532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "                      546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
       "                      560]),\n",
       "              'x': tensor([   0,   10,   19,   28,   38,   46,   50,   59,   68,   78,   84,   93,\n",
       "                       101,  111,  121,  128,  134,  140,  150,  158,  164,  173,  183,  193,\n",
       "                       200,  204,  210,  219,  227,  234,  244,  253,  261,  267,  276,  286,\n",
       "                       293,  302,  312,  322,  330,  337,  343,  351,  361,  371,  379,  389,\n",
       "                       398,  404,  414,  424,  433,  442,  450,  458,  467,  476,  482,  490,\n",
       "                       494,  498,  506,  513,  522,  530,  538,  547,  553,  562,  572,  578,\n",
       "                       587,  595,  603,  611,  621,  630,  637,  647,  656,  666,  676,  684,\n",
       "                       694,  699,  709,  719,  728,  738,  748,  758,  765,  774,  784,  794,\n",
       "                       800,  809,  819,  826,  835,  843,  853,  863,  873,  883,  893,  898,\n",
       "                       907,  916,  924,  932,  941,  949,  957,  967,  976,  986,  995, 1004,\n",
       "                      1013, 1023, 1033, 1040, 1050, 1059, 1068, 1078, 1088, 1098, 1107, 1116,\n",
       "                      1126, 1135, 1144, 1153, 1160, 1168, 1177, 1187, 1195, 1205, 1215, 1225,\n",
       "                      1235, 1242, 1251, 1261, 1269, 1279, 1288, 1296, 1305, 1313, 1322, 1330,\n",
       "                      1339, 1347, 1357, 1366, 1376, 1383, 1391, 1399, 1408, 1416, 1424, 1433,\n",
       "                      1442, 1452, 1461, 1470, 1479, 1489, 1499, 1509, 1519, 1527, 1537, 1547,\n",
       "                      1554, 1563, 1573, 1581, 1590, 1596, 1606, 1614, 1623, 1632, 1641, 1650,\n",
       "                      1659, 1668, 1677, 1687, 1697, 1707, 1717, 1727, 1733, 1743, 1752, 1760,\n",
       "                      1770, 1779, 1788, 1798, 1808, 1812, 1822, 1829, 1837, 1847, 1853, 1863,\n",
       "                      1871, 1880, 1890, 1898, 1908, 1918, 1927, 1934, 1944, 1954, 1962, 1972,\n",
       "                      1980, 1990, 1999, 2008, 2018, 2028, 2036, 2044, 2053, 2063, 2073, 2082,\n",
       "                      2092, 2096, 2105, 2115, 2125, 2134, 2143, 2153, 2163, 2166, 2175, 2184,\n",
       "                      2188, 2192, 2199, 2207, 2216, 2224, 2233, 2241, 2248, 2257, 2267, 2277,\n",
       "                      2283, 2292, 2302, 2312, 2319, 2329, 2338, 2348, 2355, 2365, 2374, 2384,\n",
       "                      2394, 2404, 2412, 2422, 2430, 2440, 2450, 2457, 2464, 2474, 2484, 2493,\n",
       "                      2503, 2512, 2519, 2528, 2538, 2545, 2555, 2565, 2572, 2582, 2589, 2591,\n",
       "                      2600, 2608, 2617, 2626, 2636, 2646, 2653, 2663, 2671, 2681, 2691, 2701,\n",
       "                      2709, 2718, 2726, 2736, 2746, 2756, 2765, 2775, 2785, 2795, 2805, 2812,\n",
       "                      2821, 2831, 2840, 2850, 2859, 2868, 2878, 2888, 2896, 2904, 2913, 2923,\n",
       "                      2933, 2943, 2953, 2963, 2972, 2982, 2992, 3002, 3009, 3017, 3023, 3032,\n",
       "                      3042, 3051, 3060, 3066, 3076, 3085, 3094, 3104, 3108, 3114, 3124, 3134,\n",
       "                      3143, 3153, 3163, 3173, 3181, 3189, 3199, 3209, 3216, 3226, 3236, 3246,\n",
       "                      3251, 3260, 3269, 3279, 3289, 3297, 3307, 3317, 3327, 3336, 3346, 3356,\n",
       "                      3366, 3375, 3385, 3395, 3405, 3414, 3421, 3428, 3436, 3444, 3454, 3464,\n",
       "                      3473, 3483, 3492, 3500, 3508, 3518, 3528, 3538, 3546, 3556, 3566, 3576,\n",
       "                      3583, 3593, 3599, 3609, 3618, 3628, 3637, 3647, 3653, 3662, 3670, 3679,\n",
       "                      3688, 3698, 3706, 3715, 3724, 3732, 3741, 3750, 3760, 3769, 3778, 3788,\n",
       "                      3798, 3808, 3818, 3827, 3834, 3842, 3850, 3860, 3868, 3872, 3879, 3887,\n",
       "                      3895, 3905, 3915, 3923, 3933, 3943, 3952, 3960, 3969, 3978, 3988, 3998,\n",
       "                      4007, 4015, 4025, 4033, 4043, 4053, 4062, 4071, 4080, 4090, 4100, 4110,\n",
       "                      4118, 4127, 4137, 4146, 4156, 4166, 4176, 4185, 4194, 4203, 4213, 4222,\n",
       "                      4232, 4240, 4250, 4258, 4268, 4278, 4287, 4297, 4307, 4317, 4326, 4336,\n",
       "                      4346, 4356, 4365, 4374, 4383, 4393, 4402, 4410, 4420, 4430, 4439, 4448,\n",
       "                      4458, 4466, 4476, 4486, 4496, 4506, 4515, 4525, 4534, 4544, 4554, 4564,\n",
       "                      4574, 4584, 4594, 4604, 4612, 4622, 4631, 4641, 4651, 4660, 4669, 4678,\n",
       "                      4687, 4697, 4705, 4714, 4723, 4733, 4743, 4752, 4762, 4772, 4782, 4792,\n",
       "                      4800, 4809, 4819, 4829, 4839, 4849, 4859, 4867, 4876, 4886, 4896, 4905,\n",
       "                      4915, 4925, 4935, 4945, 4955, 4963, 4972, 4981, 4991])}))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset\n",
    "train_data1=torch.load(\"..\\..\\downloaded\\processed\\AIDS700nef_training.pt\")\n",
    "train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches completed:   7%|▋         | 126/1904 [00:03<00:46, 37.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saiki\\IITB\\BTP1\\graphretrievaltoolkit-main\\graphretrievaltoolkit-main\\explore.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(train_loader, val_loader, model, criterion, optimizer, device)\n",
      "\u001b[1;32mc:\\Users\\saiki\\IITB\\BTP1\\graphretrievaltoolkit-main\\graphretrievaltoolkit-main\\explore.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m mean_batch_loss \u001b[39m=\u001b[39m loss_criterion(pred_sim, train_batch\u001b[39m.\u001b[39my)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Compute Gradients via Backpropagation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m mean_batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Update Parameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\saiki\\IITB\\env_general\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_loader, val_loader, model, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimGNN(\n",
       "  (convs): ModuleList(\n",
       "    (0): GCNConv(29, 64)\n",
       "    (1): GCNConv(64, 32)\n",
       "    (2): GCNConv(32, 16)\n",
       "  )\n",
       "  (attention_layer): GlobalContextAttention(input_dim=16)\n",
       "  (ntn_layer): NeuralTensorNetwork()\n",
       "  (mlp): ModuleList(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (3): Linear(in_features=8, out_features=4, bias=True)\n",
       "  )\n",
       "  (scoring_layer): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing batches completed: 100%|██████████| 307/307 [00:08<00:00, 34.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009314317466805175"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test_loss_sum=0\n",
    "with tqdm.tqdm(total=len(test_loader), desc='testing batches completed: ') as bar:\n",
    "    for batch_idx, test_batch in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_batch = test_batch.to(device)\n",
    "            pred_sim = model(test_batch.x_s, test_batch.edge_index_s, \n",
    "                    test_batch.x_t, test_batch.edge_index_t, test_batch.x_s_batch, test_batch.x_t_batch)\n",
    "            mean_test_loss = criterion(pred_sim, test_batch.y)\n",
    "            batch_test_loss_sum += mean_test_loss.item()*len(test_batch)\n",
    "\n",
    "        bar.update(1)\n",
    "\n",
    "batch_test_loss_sum/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINUX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saiki\\IITB\\BTP1\\graphretrievaltoolkit-main\\graphretrievaltoolkit-main\\explore.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saiki/IITB/BTP1/graphretrievaltoolkit-main/graphretrievaltoolkit-main/explore.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_path=\"./data\"\n",
    "train_batch_size=128\n",
    "val_batch_size=64\n",
    "test_batch_size=256\n",
    "learning_rate=0.01\n",
    "\n",
    "train_dataset = load_dataset(dpath=data_path+\"/linux/\", name=\"GED\", category=\"LINUX\", train=True)\n",
    "test_dataset = load_dataset(dpath=data_path+\"/linux/\", name=\"GED\", category=\"LINUX\", train=False)\n",
    "\n",
    "train_ged_table = train_dataset.ged[:train_dataset.data.i[-1]+1, :train_dataset.data.i[-1]+1]\n",
    "test_ged_table = test_dataset.ged[train_dataset.data.i[-1]+1:, train_dataset.data.i[-1]+1:]\n",
    "\n",
    "\n",
    "train_graph_pairs, test_graph_pairs = create_graph_pairs(train_dataset, test_dataset)\n",
    "if not osp.exists(data_path+\"/linux/graph_pairs\"):\n",
    "    os.makedirs(data_path+\"/linux/graph_pairs\")\n",
    "torch.save(train_graph_pairs, data_path+\"/linux/graph_pairs/train_graph_pairs.pt\")\n",
    "torch.save(test_graph_pairs, data_path+\"/linux/graph_pairs/test_graph_pairs.pt\")\n",
    "\n",
    "val_idxs = np.random.randint(len(train_graph_pairs), size=len(test_graph_pairs))\n",
    "val_graph_pairs = [train_graph_pairs[idx] for idx in val_idxs]\n",
    "train_idxs = set(range(len(train_graph_pairs))) - set(val_idxs)\n",
    "train_graph_pairs = [train_graph_pairs[idx] for idx in train_idxs]\n",
    "del val_idxs, train_idxs\n",
    "\n",
    "train_loader = DataLoader(train_graph_pairs, batch_size = 128, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "val_loader = DataLoader(val_graph_pairs, batch_size = 64, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "test_loader = DataLoader(test_graph_pairs, batch_size = 256, follow_batch = [\"x_s\", \"x_t\"], shuffle = True)\n",
    "\n",
    "model_linux = SimGNN(input_dim=train_loader.dataset[0].x_s.shape[-1]).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_linux.parameters(),learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 18], i=[1], x=[10, 29], num_nodes=10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path=\"./data\"\n",
    "\n",
    "train_dataset_aids = load_dataset(dpath=data_path+\"/aids/\", name=\"GED\", category=\"AIDS700nef\", train=True)\n",
    "test_dataset_aids = load_dataset(dpath=data_path+\"/aids/\", name=\"GED\", category=\"AIDS700nef\", train=False)\n",
    "train_dataset_linux=load_dataset(dpath=data_path+\"/linux/\",name=\"GED\",category=\"LINUX\",train=True)\n",
    "test_dataset_linux=load_dataset(dpath=data_path+\"/linux/\",name=\"GED\",category=\"LINUX\",train=False)\n",
    "\n",
    "train_dataset_aids[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
